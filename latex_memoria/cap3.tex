% ---------------------------------------------------------------------------------------
\chapter{Otros M\'etodos de Comparaci\'on}\label{chap3}


\section{Introducci\'on}

En la secci\'on anterior nos enfocamos en describir con detalle el Coeficiente de Informaci\'on M\'axima (MIC), esta medida de correalci\'on es una de las m\'as poderosas al momento de encontrar relaciones entre varibales, y tambi\'en una de las m\'as complejas, motivo por el cu\'al una secci\'on fue dedicada a esta. A pesar de eso, tenemos otros coeficientes tambi\'en poderosos y que nos ayduar\'an a caracterizar la relaci\'on que existe entre las im\'agenes transformadas y sus primitivas. 

En este cap\'itulo estudiaremos 3 medidas de correalci\'on, las que finalmente usaremos en la compraci\'on de las im\'agenes, estas las podemos separar en dos categorías. La primera, que corresponde a la Correlaci\'on M\'axima Local\cite{Chen2012} y la Correlaci\'on por Distancia \cite{Szekely2009}, estas son dos medidas avanzados que buscan relaciones no lineales entre conjuntos de datos. La segunda categor\'ia corresponde unicamente a la Correlaci\'on de Pearson la cu\'al es la medida m\'as utilizada para comparar dos conjuntos, pero est\'a limitada a solo encontrar relaciones lineales entre los datos.

Definiremos los coeficientes, revisaremos algunas de sus propiedades y finalmente discutiremos la inefectividad del coeficiente de Pearson para comparar im\'agenes. Luego en la secci\'on posterior estdiaremos el concepto de Equitatibilidad y lo usaremos para comparar los coeficientes. 

\section[]{Correlaci\'on M\'axima local} 

	%chen2010.pdf

	\subsection{Introducci\'on}


	La correlaci\'on local, tamb\'ien conodica como coeficiente no param\'etrico de Chen, o coeficiente de Chen, es un coeficiente que busca detectar relaciones no lineales basado en la integral de correlaci\'on. Este fue propuesto por Chen et al. en su trabajo \textit{A Nonparametric Approach to Detect Nonlinear Correlation in Gene Expression} (2012) \cite{Chen2012}. Este coeficiente es una medida de asociaci\'on no param\'etrica que puede detectar relaciones no lineales entre dos variables, esto lo hace a trav\'es de un m\'etodo similar al de aproximaci\'on lineal por secci\'ones. 

	La integral de correlaci\'on examina la distribuci\'on acumulativa de distancias entre puntos en una serie de tiempo, en base a esto y con algunas modificaciones, se puede utilizar para estimar patrones y asociaciones globales. 


	\subsection{Definiciones}

	Defnimamos primero el coraz\'on del coeficiente, la integral de correlaci\'on:

	\begin{defn}[Correlaci\'on Integral]
		Sea $z_1,\dots,z_N$ una serie de tiempo de $N$ observaciones y sea $r\in\R_{>0}$. Definimos la integral de correlaci\'on $I(r)$ una funci\'on indicadora definida como:
		$$
		I(r)=\lim _{N \rightarrow \infty}\left\{\frac{1}{N^{2}} \sum_{i, j=1}^{N} I\left(\left|z_{i}-z_{j}\right|<r\right)\right\}.
		$$
	\end{defn}

	La integral de correlaci\'on cuantifica el el n\'umero promedio de vecinos dentro de un radio $r$. Notemos que esta definici\'on sigue teniendo sentido cuando los datos no son series de tiempo, solo requerimos de una indexaci\'on de los estos.

	Para desarrollar una medida de asociaci\'on entre vectores, $x$ e $y$, modificamos la definici\'on de $I(r)$ como sigue. Sean $z_i=(x_i,y_i)$ con $i=1,\dots, N$ las observaciones en el conjunto de datos. Con esto definimos:

	\begin{defn}[Correlaci\'on Integral entre vectores]
		Sean $x= \{x_1,\dots,x_N\} $ e $y=\{y_1,\dots,y_N\}$ dos vectores de $N$ observaciones y sea $r\in\R_{>0}$. Definimos la integral de correlaci\'on $\hat{I}(r)$ entre $x$ e $y$ como:
		$$\hat{I}(r)=\frac{1}{N^{2}} \sum_{i, j=1}^{N} I\left(\left|\begin{pmatrix}
			x_{i} \\
			y_{i}
		  \end{pmatrix}-\begin{pmatrix}
			x_{j} \\
			y_{j}
		  \end{pmatrix}\right|<r\right)$$,
		donde $|\cdot|$ es la norma euclidiana. En adelante, al referirnos a la Integral de Correlaci\'on nos referiremos a esta definici\'on, a menos que se indique lo contrario.

	\end{defn}
	
	Notemos que las distancias obsevadas son adem\'as linealmente transformadas para que se encuentren entres 0 y 1 antes de calcular $\hat{I}$. Es importante destacar que $\hat{I}$a tiene las propiedades de una funci\'on de distribuci\'on acumulativa. Es no decreciente entre 0 y 1 y continua por la derecha. La funci\'on $\hat{I}(r)$ descrive el patr\'on global de distancias entre vecinos. 

	El inter\'es principal es la definici\'on de una metrica para cuantificar la asosiaci\'on no lineal estudiando patrones locales. Dado esto, definimos la densidad de vecinos $D$ de forma similar a la derivada de $\hat{I}$: 
	\begin{defn} [Densidad de Vecinos]
		Dados $x= \{x_1,\dots,x_N\} $, $y=\{y_1,\dots,y_N\}$ y su respectiva Integral de correalci\'on $\hat{I}(r)$
		$$
		\hat{D}(r)= \frac{\vartriangle\hat{I}(r)}{\vartriangle r},
		$$
		donde $\vartriangle\hat{I}(r)$ denota un cambio en $\hat{I}(r)$ y $\vartriangle r$ la magnitud de este.
	\end{defn}
	
	La densidad de vecinos observada es evaluada en radio distreto r, con $r=0,1/m, 2/m, \dots, 1$, con $m$ es un grosor de malla arbitrario que determina $\vartriangle r = \frac{1}{m}$. Una funci\'on de suavizado autom\'atico es usado para suavizar $D(r)$, en su propuesta original se us\'o el algoritmo propuesto por Videla et al. (2007) \cite{videla2007}, nosotros utilizaremos filtros Savitzky-Golay \cite{Savitzky-Golay}, tambi\'en haciendo uso de validaci\'on cruzada para elegir los mejores hiperparametros. En el paper, el tama\~no predeterminado $m$ se establece como $N$, el n\'umero de observaciones y en este trabajo usaremos el mismo $m$. El estad\'istico $\hat{D}$ es una aproximaci\'on discreta de $d\hat{I}(r)/d r$, la cual tiene las propiedades formales de una probabilidad funci\'on de densidad. Por lo tanto, con un ligero abuso de terminolog\'ia nos referimos a $\widehat{D}(r)$ como una distribuci\'on.

	En base a esto definimos la correlaci\'on local. Intuitivamente, las distancias entre los puntos de datos entre dos variables correlacionadas diferir\'ian de las distancias entre dos variables no correlacionadas. 
	
	\begin{defn}[Correlaci\'on Local]
	
		Sea $\widehat{D_0}(r)$ la estimaci\'on de una distribuci\'on nula, que se compone de dos vectores sin asociaci\'on. Definimos la correlaci\'on local ($\ell(r)$) como la desviaci\'on de D de la de la distribuci\'on nula a una distancia vecina dada r:

		$$
			\ell(r)=\widehat{D}(r)-\widehat{D}_{0}(r),
		$$	

		donde $\widehat{D}(r)$ es la distribuci\'on asociada a los vectores que queremos comparar.

	\end{defn}

	Este enfoque no asume ninguna distribuci\'on param\'etrica. La flexibilidad de este m\'etodo facilita el cambio de la distribuci\'on nula a cualquier distribuci\'on de inter\'es. 
	
	Por ultimo, definimos el coficiente como de correlaci\'on local m\'axima, o coeficiente de Chen como:

	\begin{defn}
		Sea $\ell(r)$ la funci\'on de correlaci\'on local entre dos vectores, definimos el coeficiente de correlaci\'on local m\'axima como:

		$$
		M=\max _{r}\{|\ell(r)|\}
		$$

		con $r\in[0,1]$. Recordemos que las distancias fueron nomralizadas.
		
	\end{defn}
	La interpretaci\'on de $\ell(r)$ como la diferencia de dos distribuciones implica que $M$ puede interpretarse como la distancia bajo la norma del supremo entre $\widehat{D}$ y $\widehat{D_0}$. En otras palabras, definimos el estad\'istico M como la desviaci\'on m\'axima entre dos densidades vecinas subyacentes.

\section[]{\textit{Distance Covariance \& Distance Correlation}} 

\subsection{Introducci\'on}


La Covarianza por Distancia  y la Correlaci\'on por distancia  fueron propuestas por Sz\'ekely, Rizzo, y Bakirov en su trabajo \textit{''Measuring and testing independence by correlation of distances''} (2007) \cite{Szekely2007}, posteriormente tambi\'en propusieton la Covarianza por Distancia Browniana en su trabajo \textit{''Brownian Distance Covariance''} (2009) \cite{Szekely2009}, en donde tambi\'en se demostr\'o que esta coincide con la Covarianza por Distancia tradicional.

De la misma forma que las medidas que hemos visto anteriormente, estas son medidas de asociaci\'on no param\'etricas que buscan encontrar relaciones no lineales entre dos conjuntos de datos, en particular establecer una forma de caracterizar independenc\'ia entre dos distribuci\'ones. Dado esto es importante recalcar que, para distribuci\'on de primer momento fin\'ito, la Correlaci\'on por distancia ($\mathcal{R}$) generaliza la idea de correalci\'on en dos formas:

\begin{enumerate}
	\item $\mathcal{R}(X,Y)$ est\'a definido para $X,Y$ de dimensi\'on aleatoria.
	\item $\mathcal{R}(X,Y) = 0$ caracteriza la independenc\'a de $X$ e $Y$.	 
\end{enumerate}

La primera de estas afirmaciones es importante, ya que nos permite utulizar esta medida para comparar im\'agenes sin mayor problema, de todas formas en la secci\'on \ref{chap5} estudiaremos como adaptar est\'as medidas para im\'agnes. En esta secci\'on definiremos estos 3 coeficientes, la relaci\'on entre ellos y verificaremos la afirmaciones realizadas m\'as arriba, para esto comenzaremos con las definiciones. 

\subsection{Definiciones}

Sean $X$ en $\R^p$ y $Y$ en $\R^q$ vectores aleatorios, donde $p$ y $q$ son enteros positivos. Las funciones en min\'uscula $f_X$ y $f_Y$ se utilizarán para denotar las funciones caracter\'isticas de $X$ y $Y$, respectivamente, y su funci\'on caracter\'istica conjunta se denota como $f_{X, Y}$. 

\begin{defn}[Covarianza por distancia]
	La Covarianza por distancia (dCov) entre los vectores aleatorios $X$ e $Y$ con primeros momentos finitos es el n\'umero no negativo $\mathcal{V}(X, Y)$ definido por:
	\begin{equation}
		\begin{aligned}\label{dcov_formula}
			\mathcal{V}^2(X, Y) & =\left\|f_{X, Y}(t, s)-f_X(t) f_Y(s)\right\|^2 \\
			& =\frac{1}{c_p c_q} \int_{\R^{p+q}} \frac{\left|f_{X, Y}(t, s)-f_X(t) f_Y(s)\right|^2}{|t|_p^{1+p}|s|_q^{1+q}} d t d s .
			\end{aligned}
	\end{equation}

	\end{defn}

	Similarmente, la Varianza por distancia (dVar) se define como la raiz cuadrada:

	$$
	\mathcal{V}^2(X)=\mathcal{V}^2(X, X)=\left\|f_{X, X}(t, s)-f_X(t) f_X(s)\right\|^2 .
	$$

	Por definici\'on de la norma $\|\cdot\|$, es claro que  $\mathcal{V}(X, Y) \geq 0$ y $\mathcal{V}(X, Y)=0$ si y solo si $X$ e $Y$ son indepentes.

	\begin{defn} [Correlaci\'on por distancia]
		La correlaci\'on por distancia (dCor) entre los vectores aleatorios $X$ e $Y$ con primer momento finito es el n\'unero no negtivo $\mathcal{R}(X, Y)$ definido por:
		$$
		\mathcal{R}^2(X, Y)= \begin{cases}\frac{\mathcal{V}^2(X, Y)}{\sqrt{\mathcal{V}^2(X) \mathcal{V}^2(Y)}}, & \mathcal{V}^2(X) \mathcal{V}^2(Y)>0 \\ 0, & \mathcal{V}^2(X) \mathcal{V}^2(Y)=0\end{cases}.
		$$

	Algunas propiedades de $\mathcal{R}$ an\'alogo a $\rho$ ser\'an revisadas m\'as adelante. 
	\end{defn}

	Ahora, para el caso muestral, definimos los estad\'isticos de distancia de la siguiente forma. Para una muestra aleatoria $(\mathbf{X}, \mathbf{Y})=\left\{\left(X_k, Y_k\right): k=1, \ldots, n\right\}$ de $n$ ventores aleatorios $(X, Y)$ i.i.d., calculamos las matrices de distancia euclideana  $\left(a_{k l}\right)=\left(\left|X_k-X_l\right|_p\right)$ y $\left(b_{k l}\right)=\left(\left|Y_k-Y_l\right|_q\right)$. Definimos:

	$$
	A_{k l}=a_{k l}-\bar{a}_{k .}-\bar{a}_{. l}+\bar{a}_{. .}, \quad k, l=1, \ldots, n,
	$$
	donde
	$$
	\bar{a}_{k .}=\frac{1}{n} \sum_{l=1}^n a_{k l}, \quad \bar{a}_{. l},=\frac{1}{n} \sum_{k=1}^n a_{k l}, \quad \bar{a}_{. .}=\frac{1}{n^2} \sum_{k, l=1}^n a_{k l} .
	$$
	
	De forma similar, definimos $B_{k l}=b_{k l}-\bar{b}_{k .}-\bar{b}_{\cdot l}+\bar{b}_{. .}$, para $k, l=1, \ldots, n$.

	\begin{defn}[Covarianza y Correlaci\'on por distancia muestral]
		La no negativa Covarianza por distancia muestral $\mathcal{V}_n(\mathbf{X}, \mathbf{Y})$ y la Correlaci\'on por distancia muestral $\mathcal{R}_n(\mathbf{X}, \mathbf{Y})$ estan definidas por
		$$
		\mathcal{V}_n^2(\mathbf{X}, \mathbf{Y})=\frac{1}{n^2} \sum_{k, l=1}^n A_{k l} B_{k l},
		$$
		y
		$$
		\mathcal{R}_n^2(\mathbf{X}, \mathbf{Y})= \begin{cases}\frac{\mathcal{V}_n^2(\mathbf{X}, \mathbf{Y})}{\sqrt{\mathcal{V}_n^2(\mathbf{X}) \mathcal{V}_n^2(\mathbf{Y})}}, & \mathcal{V}_n^2(\mathbf{X}) \mathcal{V}_n^2(\mathbf{Y})>0 \\ 0, & \mathcal{V}_n^2(\mathbf{X}) \mathcal{V}_n^2(\mathbf{Y})=0\end{cases},
		$$
		respectivamente. Adem\'as la Varianza por distancia muestral $\mathcal{V}_n(\mathbf{X})$ est\'a definida por
		$$
		\mathcal{V}_n^2(\mathbf{X})=\mathcal{V}_n^2(\mathbf{X}, \mathbf{X})=\frac{1}{n^2} \sum_{k, l=1}^n A_{k l}^2 .
		$$
	\end{defn}

	Por ultimo, plantearemos los siguientes teoremas que nos entregan  propiedades importantes de la Correlaci\'on por distancia:

	\begin{thm}
		Si $(\mathbf{X}, \mathbf{Y})$ es una muestra aleatoria del vector aleatorio $(X, Y)$, entonces
		$$
		\mathcal{V}n^2(\mathbf{X}, \mathbf{Y})=\left\|f{X, Y}^n(t, s)-f_X^n(t) f_Y^n(s)\right\|^2 .
		$$
	\end{thm}
	\begin{thm}
		Si $E|X|_p<\infty$ and $E|Y|_q<\infty$, entonces casi seguramente
		$$
		\lim _{n \rightarrow \infty} \mathcal{V}_n(\mathbf{X}, \mathbf{Y})=\mathcal{V}(X, Y) .
		$$
	\end{thm}
	
	\begin{cor}
		If $E\left(|X|_p+|Y|_q\right)<\infty$, then almost surely
		$$
		\lim _{n \rightarrow \infty} \mathcal{R}_n^2(\mathbf{X}, \mathbf{Y})=\mathcal{R}^2(X, Y) \text {. }
		$$
	\end{cor}
	\begin{thm}\label{thm3_brcov}
		Para vectores aleatorios $X \in \mathbb{R}^p$ y $Y \in \mathbb{R}^q$ tal que $E\left(|X|_p+\right.$ $\left.|Y|_q\right)<\infty$, se tienen las siguientes propiedades:
		\begin{enumerate}
			\item $0 \leq \mathcal{R}(X, Y) \leq 1$, y $\mathcal{R}=0$ si y solo si $X$ e $Y$ son independientes.
			\item $\mathcal{V}\left(a_1+b_1 C_1 X, a_2+b_2 C_2 Y\right)=\sqrt{\left|b_1 b_2\right|} \mathcal{V}(X, Y)$, para todos los vectores constantes $a_1 \in \mathbb{R}^p, a_2 \in \mathbb{R}^q$, escalares $b_1, b_2$ y matrices ortogonales $C_1, C_2$ en $\mathbb{R}^p$ y $\mathbb{R}^q$, respectivamente.	
			\item Si el vector aleatorio $\left(X_1, Y_1\right)$ es ind. del vector aleatorio $\left(X_2, Y_2\right)$, se tiene
			 $$
			\mathcal{V}\left(X_1+X_2, Y_1+Y_2\right) \leq \mathcal{V}\left(X_1, Y_1\right)+\mathcal{V}\left(X_2, Y_2\right) .
			$$
			La igualdad se cumple si y solo si $X_1$ e $Y_1$ son constantes, o $X_2$ e $Y_2$ son constantes, o $X_1, X_2, Y_1, Y_2$ son mutuamente independientes.

			\item $\mathcal{V}(X)=0$ implica que $X=E[X]$, casi seguramente.
			\item $\mathcal{V}(a+b C X)=|b| \mathcal{V}(X)$, para todos los vectores constantes $a$ en $\mathbb{R}^p$, escalares $b$, y matrices ortogonales $p \times p$ $C$.
			\item Si $X$ e $Y$ son independientes, entonces $\mathcal{V}(X+Y) \leq \mathcal{V}(X)+\mathcal{V}(Y)$. La igualdad se cumple si y solo si uno de los vectores aleatorios $X$ o $Y$ es constante.
		\end{enumerate}
	\end{thm}
	\begin{thm}
	\begin{enumerate}
		\item $\mathcal{V}_n(\mathbf{X}, \mathbf{Y}) \geq 0$.
		\item $\mathcal{V}_n(\mathbf{X})=0$ si y solo si cada observaci\'on de la muestra es id\'entica.
		\item $0 \leq \mathcal{R}_n(\mathbf{X}, \mathbf{Y}) \leq 1$.
		\item $\mathcal{R}_n(\mathbf{X}, \mathbf{Y})=1$ implica que las dimensiones de los subespacios lineales generados por $\mathbf{X}$ y $\mathbf{Y}$ respectivamente son casi seguramente iguales, y si asumimos que estos subespacios son iguales, entonces en este subespacio
		\item $$
		\mathbf{Y}=a+b \mathbf{X} C,
		$$
		para alg\'un vector $a$, n\'umero real no nulo $b$, y matriz ortogonal $C$.
	\end{enumerate}	
	\end{thm}
	demostraci\'ones de todos estos teoremas pueden ser encontrados en Szekely y Rizzo (2009) \cite{Szekely2009}.

	\subsection{Covarianza Browniana}
		
		Para introducir la noci\'on de covarianza browniana, comenzaremos considerando la covarianza cuadrada del producto-momento. Recordemos que una variable prima $X'$ denota una copia independiente e id\'enticamente distribuida (i.i.d.) del símbolo no prima $X$. Para dos variables aleatorias continuas de valores reales, el cuadrado de su covarianza clásica es:

		$$\E[(X-\E(X))(Y-\E(Y))] = \E[(X-\E(X))(X'-\E(X'))(Y-\E(Y))(Y'-\E(Y'))]	$$

		Ahora generalizamos la covarianza al cuadrado y definimos el cuadrado de la covarianza condicional, dadas dos procesos estocásticos de valores reales, $U(\circ)$ y $V(*)$. Obtenemos un resultado interesante cuando U y V son procesos de Wiener independientes.


		Primero, para centrar la variable aleatoria $X$ en la covarianza condicional, necesitamos la siguiente definici\'on. Sea $X$ una variable aleatoria de valores reales y $\{U(t): t \in \R\}$ un proceso estoc\'astico de valores reales, independiente de $X$. La versi\'on centrada de $X$ respecto a $U$ se define por:

		$$
		X_U = U(X)-\int_{-\infty}^{\infty}U(t)dF_X(t)=U(X)-\E[U(X)|U],
		$$

		siempre que la esperanza condicional exista. Notemos que, si $id$ es la identidad, $X_{id}=X-\E[X]$. 

		Ahora, sea $\mathcal{W}$ un movimiento Browniano/Wiener de una dimensi\'on en ambos lados con esperanza cero y funci\'on de covarianza.
		\begin{equation}\label{brw_covariamce}
			|s|+|t|-|s-t|=2\min(s.t)
		\end{equation}
			

		Esto es dos veces la covarianza estandar para un proceso Wiener. Pero en adelante el factor de dos nos simplificar\'a algunos calculos, por lo que en el resto de la secci\'on asumiermos esta funci\'on de covarianza \ref{brw_covariamce} para $\mathcal{W}$. Ya con esto, podemos definir la Covarianza Browniana 

		\begin{defn}[Covarianza Browniana]
			La Covarianza Browniana (o covarianza Wiener) de dos variables aleatorias con valores reales $X$ e $Y$, con segundos monetos fin\'itos es el n\'umero no negativo definido por su cuadrado:
			\begin{equation}\label{brw_cov}
				\mathcal{W}^2(X,Y)={Cov}_{{W}^2}(X,Y)=\E[X_{W}X'_{W}Y_{W'}Y'_{W'}],
			\end{equation}

			donde $({W},{W'})$ no dependen de $(X,X',Y.Y')$.
		\end{defn}

		Tomemos en cuenta que si ${W}$ en ${Cov}_{{W}^2}$ es reemplazada por la funci\'on identidad (no aleatoria) $id$, entonces ${Cov}_{id}(X, Y) = |{Cov}(X, Y)| = |\sigma_{X,Y}|$, el valor absoluto de la covarianza del producto-momento de Pearson. Mientras que la covarianza del producto-momento estandarizada, la correlación de Pearson ($\rho$), mide el grado de relaci\'on lineal entre dos variables con valores reales, veremos que la covarianza estandarizada de Brownian mide el grado de todos los tipos posibles de relaciones entre dos variables aleatorias con valores reales.

		La definición de ${Cov}_{{W}^2}$ se puede extender a procesos aleatorios en dimensiones superiores de la siguiente manera. Si $X$ es una variable aleatoria con valores en $\R^p$ y $U(s)$ es un proceso aleatorio (campo aleatorio) definido para todos los $s \in \R^p$ e independiente de $X$, define la versión centrada en $U$ de $X$ como

		$$
			X_U=\E(X)-\E[U(X)|U]
		$$
		siempre que la expectativa condicional exista.

		\begin{defn}
			Si $X$ es un vector aleatorio con valores en $\R^p$ y $Y$ es un vector aleatorio con valores en $\R^q$, y $U(s)$ y $V(t)$ son procesos aleatorios, definamos para todo $s\in\R^p$, $t\in\R^q$, entonces la covarianza (U,V) de $X$ e $Y$ es el n\'umero no negativo definido por su cuadrado:
			\begin{equation}\label{eq_35_brcov}
				{Cov}^2_{(U,V)}(X,Y) = \E[X_U X'_U Y_V Y'_V],
			\end{equation}
				
			siempre que el lado derecho sea positivo y finito.
		\end{defn}

		En particular, si ${W}$ y ${W'}$ son procesos brownianos con funci\'on de covarianza \ref{brw_covariamce} en $\R^p$ y $\R^q$ respectivamente, la Covarianza Browniana de $X$ e $Y$ esta definida por:

		\begin{equation}
			\mathcal{W}^2(X,Y) = Cov^2_W(X,Y) = Cov^2_{W,W'}(X,Y).
		\end{equation}

		De la misma forma, para variables aleatorias con varianza finita podemos definir la Varianza Browniana:

		\begin{equation}
			\mathcal{W}(X) = Cov_W(X) = Cov_{W}(X,X).
		\end{equation}
		
		Y con esto, podemos definir la correlaci\'on Browniana:
		
		\begin{defn}[Correlaci\'on Browniana]
			Definimos la correalci\'on Browniana de dos variables aleatorias con valores reales $X$ e $Y$ con segundos momentos finitos como:
			\begin{equation}	
				Cor_W(X,Y) = \frac{\mathcal{W}}{\sqrt[]{\mathcal{W}(X)\mathcal{W}(Y)}},
			\end{equation}
			siempre que el denominador no sea 0; otro caso $Cor_W(X,Y) = 0$.
		\end{defn}

		Ahora, solo nos queda probar que $Cov_W(X,Y)$ existe para vectores aleatorios de segundos momentos finitos, y con esto derivar la Covarianza Browniana. Para esto, tenemos el siguiente teorema:

		\begin{thm}\label{thm_exists_cov_browniana}
			Si $X$ es un vector aleatorio con valores en $\R^p$, e $Y$ es un vector aleatorio con valores en $\R^q$, y $\E(|X|^2+|Y|^2)>\infty$, entonces $\E[X_{W}X'_{W}Y_{W'}Y'_{W'}]$ es no negativo y finito, y:
			\begin{align}
				\mathcal{W}^2(X,Y) &= \E[X_{W}X'_{W}Y_{W'}Y'_{W'}]\\
								   &= \E|X-X'||Y-Y'| + \E|X-X'|\E|Y-Y'| - \label{cov_browniana_exists} \\ 
								   &\qquad - \E|X-X'||Y-Y''|\E|X-X''||Y-Y'|,\label{eq_37_brcov}
 			\end{align}
			donde $(X,Y)$, $(X',Y')$, y $(X'',Y'')$ son iid.
		\end{thm}
		\begin{proof}
			Notemos que
			$$
			\begin{aligned}
			E\left[X_W X_W^{\prime} Y_{W^{\prime}} Y_{W^{\prime}}^{\prime}\right] & =E\left[E\left(X_W Y_{W^{\prime}} X_W^{\prime} Y_{W^{\prime}}^{\prime} \mid W, W^{\prime}\right)\right] \\
			& =E\left[E\left(X_W Y_{W^{\prime}} \mid W, W^{\prime}\right) E\left(X_W^{\prime} Y_{W^{\prime}}^{\prime} \mid W, W^{\prime}\right)\right] \\
			& =E\left[E\left(X_W Y_{W^{\prime}} \mid W, W^{\prime}\right)\right]^2,
			\end{aligned}
			$$
			y esto es siempre no negativo. Para finitud, es suficiente demostrar que todos los factores en la definici\'on de  $\operatorname{Cov}_W(X, Y)$ tienen cuartos momentos finitos. La ecuaci\'on \ref{cov_browniana_exists} se basa en la forma especial de la funci\'on de covarianza de \ref{brw_covariamce} de $W$. El resto de la demostraci\'on se puede encontrar en el ap\'endice de /Szehely and Rizzo (2009)\cite{Szekely2009}.	
		\end{proof}

	\subsection{Equivalencia entre covarianzas}
	
	Ahora estudiaremos la equivalencia entre la Covarianza por distancia y la Covarianza Browniana. Pero antes de esto necesitamos plantear el siguiente lema:

	\begin{lem}\label{lemma_1_brcov}
		Si $0<\alpha<2$, entonces para todo $x$ en $\R^d$
		$$
		\int_{\R^d} \frac{1-\cos \langle t, x\rangle}{|t|_d^{d+\alpha}} d t=C(d, \alpha)|x|_d^\alpha,
		$$
		donde
		$$
		C(d, \alpha)=\frac{2 \pi^{d / 2} \Gamma(1-\alpha / 2)}{\alpha 2^\alpha \Gamma((d+\alpha) / 2)},
		$$
		y $\Gamma(\cdot)$ es la funci\'on completea gamma. Las integrales en 0 y $\infty$ se entienden en el sentido de valor principal: $\lim_{\varepsilon \rightarrow 0} \int_{\R^d \backslash\left\{\varepsilon B+\varepsilon^{-1} B^c\right\}}$, donde $B$ es la bola unitaria (centrada en 0) en $\R^d$ y $B^c$ es el complemento de $B$.
	\end{lem}
	Una demostraci\'on de este lemma puede ser entontrada en Székely and Rizzo (2007) \cite{Szekely2007}. El Lemma \ref{lemma_1_brcov}
	sugiere que la funci\'on de pesos
	\begin{equation}\label{eq_23_brcov}
	w(t, s ; \alpha)=\left(\left.\left.C(p, \alpha) C(q, \alpha)|t|_p^{p+\alpha}\right|s\right|_q ^{q+\alpha}\right)^{-1}, \quad 0<\alpha<2 .	
	\end{equation}

	En el caso m\'as simple correspondiente a $\alpha=1$ y norma euclidiana $|x|$,
	\begin{equation}\label{eq_24_brcov}
		w(t, s)=\left(c_p c_q|t|_p^{1+p}|s|_q^{1+q}\right)^{-1},
		\end{equation}
	
	donde,
	\begin{equation}\label{eq_25_brcov}
		c_d=C(d, 1)=\frac{\pi^{(1+d) / 2}}{\Gamma((1+d) / 2)} .
	\end{equation}
	
	(La constante $2 c_d$ es el \'area de la esfera unitaria en $\R^{d+1}$).
	\begin{rem}
		El Lemma \ref{lemma_1_brcov} es aplicado para evaluar el integrando en \ref{dcov_formula} para las funciones de peso \ref{eq_23_brcov} y \ref{eq_24_brcov}. Por ejemplo, si $\alpha=1$ \ref{eq_24_brcov}, entonces por el Lemma \ref{lemma_1_brcov} existen constantes $c_p$ y $c_q$ tal que para $X$ en $\R^p$ y $Y$ en $\R^q$,
		$$
		\begin{gathered}
		\int_{\mathbb{R}^p} \frac{1-\exp \{i\langle t, X\rangle\}}{|t|p^{1+p}} d t=c_p|X|_p, \quad \int{\mathbb{R}^q} \frac{1-\exp \{i\langle s, Y\rangle\}}{|s|_q^{1+q}} d s=c_q|Y|_q, \\
		\int_{\mathbb{R}^p} \int_{\mathbb{R}^q} \frac{1-\exp \{i\langle t, X\rangle+i\langle s, Y\rangle\}}{|t|_p^{1+p}|s|_q^{1+q}} d t d s=c_p c_q|X|_p|Y|_q
		\end{gathered}
		$$
	\end{rem}
	
	Ya con esto, podemos pasar el resultado principal de esta secci\'on, con el siguiente teorema.
	

	\begin{thm}\label{thm_8_brcov}
		Para un $X \in \R^p, Y \in \R^q$ arbitrario, con segundos momentos finitos
			$$
			\mathcal{W}(X, Y)=\mathcal{V}(X, Y) .
			$$
			\end{thm}
	
	\begin{proof}
		
	Proof. Ambos $\mathcal{V}$ y $\mathcal{W}$ son no negativos, por tanto, basta por demostrar que sus cuadrados coinciden. Para esto, primero notemos Lemma \ref{lemma_1_brcov} puede ser aplicado para evaluar $\mathcal{V}^2(X, Y)$. En el numerador de la integral tenemos t\'erminos como:
	
	$$
	E\left[\cos \left\langle X-X^{\prime}, t\right\rangle \cos \left\langle Y-Y^{\prime}, s\right\rangle\right],
	$$

	donde $X, X^{\prime}$ son i.i.d. y $Y, Y^{\prime}$ son i.i.d. Ahora, aplicando la identidad:
	$$
	\cos u \cos v=1-(1-\cos u)-(1-\cos v)+(1-\cos u)(1-\cos v)
	$$
	y el Lemma \ref{lemma_1_brcov} para simplificar el integrando. Despu\'es de cancelar en el numerador del integrando, queda por evaluar integrales del tipo:

	$$
	\begin{aligned}
	& E \int_{\R^{p+q}} \frac{\left.\left[1-\cos \left\langle X-X^{\prime}, t\right\rangle\right]\left[1-\cos \left\langle Y-Y^{\prime}, s\right\rangle\right)\right]}{|t|^{1+p}|s|^{1+q}} d t d s \\
	& \quad=E\left[\int_{\R^p} \frac{1-\cos \left\langle X-X^{\prime}, t\right\rangle}{|t|^{1+p}} d t \times \int_{\R^q} \frac{1-\cos \left\langle Y-Y^{\prime}, s\right\rangle}{|s|^{1+q}} d s\right] \\
	& =c_p c_q E\left|X-X^{\prime}\right| E\left|Y-Y^{\prime}\right|
	\end{aligned}
	$$
	Luego de esto, y simplificando, obtenemos:
	$$
	\begin{aligned}
	\mathcal{V}^2(X, Y)= & E\left|X-X^{\prime}\right|\left|Y-Y^{\prime}\right|+E\left|X-X^{\prime}\right| E\left|Y-Y^{\prime}\right| \\
	& -E\left|X-X^{\prime}\right|\left|Y-Y^{\prime \prime}\right|-E\left|X-X^{\prime \prime}\right|\left|Y-Y^{\prime}\right|,
	\end{aligned}
	$$
	y esta es exactamente igual a la expresi\'on \ref{eq_37_brcov} obtenida para $\mathcal{W}^2(X, Y)$ en el Teorema \ref{thm_exists_cov_browniana}.
	\end{proof}

	Como corolario del Teorema \ref{thm_8_brcov}, tenemos que las propiedades de la Covarianza Browniana para vectores aleatorios $X$ e $Y$ con segundos momentos finitos son las mismas propiedades establecidas para la Covarianza por distancia $\mathcal{V}(X, Y)$ en el Teorema \ref{thm3_brcov}.
	
	El sorpredente resultado de que la Covarianza Browniana es igual a la Covarianza por distancia, exactamente como se define en \ref{dcov_formula} para $X \in \R^p$ e $Y \in \R^q$, es paralelo a un caso especial familiar cuando $p=q=1$. Para bivariados $(X, Y)$ encontramos que $\mathcal{R}(X, Y)$ es un contraparte natural del valor absoluto de la correlaci\'on de Pearson. Es decir, si en \ref{eq_35_brcov} $U$ y $V$ son la funci\'on no aleatoria m\'as simple $id$, entonces obtenemos el cuadrado de la covarianza de Pearson $\sigma_{X, Y}^2$. Luego, si consideramos los procesos aleatorios m\'as fundamentales, $U=W$ y $V=W^{\prime}$, obtenemos el cuadrado de la covarianza por distancia, $\mathcal{V}^2(X, Y)$.

\section[]{Correlaci\'on de Pearson}

	\subsection{Introducci\'on}

	\todo{donde se public\'o, pequena intro.}
		
	 
	\section{Definiciones}
	
	El coef. se define como:
	\begin{equation}\label{pearson_orig}
		\rho_{X,Y}=\frac{cov(X,Y)}{\sigma_X\sigma_Y}
	\end{equation}
	
	Para una muestra de tama\~no $N$, tenemos:
	
	\begin{equation}\label{pearson_r}
		r=\frac{\sum_{i}^N\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sqrt{\sum_{i}^n\left(x_{i}-\bar{x}\right)^{2}} \sqrt{\sum_{i}^n\left(y_{i}-\bar{y}\right)^{2}}}
	\end{equation}
	
	Con $x_i,y_i$ elementos de la muestra y $\bar{x},\bar{y}$ sus respectivos promedios.
	\todo{Hablar de The Ineffectiveness of the Correlation Coefficient for Image ComparisonsS}

\section{Ejemplos}

	\todo{usar datasaurus para mostrar los coeficientes}

	\newpage