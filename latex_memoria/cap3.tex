% ---------------------------------------------------------------------------------------
\chapter{Otros M\'etodos de Comparaci\'on de Im\'agenes}\label{chap3}


\section{Introducci\'on}

En la secci\'on anterior nos enfocamos en describir con detalle el Coeficiente de Informaci\'on M\'axima (MIC), esta medida de correalci\'on es una de las m\'as poderosas al momento de encontrar relaciones entre varibales, y tambi\'en una de las m\'as complejas, motivo por el cu\'al una secci\'on fue dedicada a esta. A pesar de eso, tenemos otros coeficientes tambi\'en poderosos y que nos ayduar\'an a caracterizar la relaci\'on que existe entre las im\'agenes transformadas y sus primitivas. 

En este cap\'itulo estudiaremos 3 medidas de correalci\'on, las que finalmente usaremos en la compraci\'on de las im\'agenes, estas las podemos separar en dos categorías. La primera, que corresponde a la Correlaci\'on M\'axima Local\cite{Chen2012} y la Correlaci\'on por Distancia \cite{Szekely2009}, estas son dos medidas avanzados que buscan relaciones no lineales entre conjuntos de datos. La segunda categor\'ia corresponde unicamente a la Correlaci\'on de Pearson la cu\'al es la medida m\'as utilizada para comparar dos conjuntos, pero est\'a limitada a solo encontrar relaciones lineales entre los datos.

Definiremos los coeficientes, revisaremos algunas de sus propiedades y finalmente discutiremos la inefectividad del coeficiente de Pearson para comparar im\'agenes. Luego en la secci\'on posterior estdiaremos el concepto de Equitatibilidad y lo usaremos para comparar los coeficientes. 

\section[]{Correlaci\'on M\'axima local} 

	%chen2010.pdf

	\subsection{Introducci\'on}


	La correlaci\'on local, tamb\'ien conodica como coeficiente no param\'etrico de Chen, o coeficiente de Chen, es un coeficiente que busca detectar relaciones no lineales basado en la integral de correlaci\'on. Este fue propuesto por Chen et al. en su trabajo \textit{A Nonparametric Approach to Detect Nonlinear Correlation in Gene Expression} (2012) \cite{Chen2012}. Este coeficiente es una medida de asociaci\'on no param\'etrica que puede detectar relaciones no lineales entre dos variables, esto lo hace a trav\'es de un m\'etodo similar al de aproximaci\'on lineal por secci\'ones. 

	La integral de correlaci\'on examina la distribuci\'on acumulativa de distancias entre puntos en una serie de tiempo, en base a esto y con algunas modificaciones, se puede utilizar para estimar patrones y asociaciones globales. 


	\subsection{Definiciones}

	Defnimamos primero el coraz\'on del coeficiente, la integral de correlaci\'on:

	\begin{defn}[Correlaci\'on Integral]
		Sea $z_1,\dots,z_N$ una serie de tiempo de $N$ observaciones y sea $r\in\R_{>0}$. Definimos la integral de correlaci\'on $I(r)$ una funci\'on indicadora definida como:
		$$
		I(r)=\lim _{N \rightarrow \infty}\left\{\frac{1}{N^{2}} \sum_{i, j=1}^{N} I\left(\left|z_{i}-z_{j}\right|<r\right)\right\}.
		$$
	\end{defn}

	La integral de correlaci\'on cuantifica el el n\'umero promedio de vecinos dentro de un radio $r$. Notemos que esta definici\'on sigue teniendo sentido cuando los datos no son series de tiempo, solo requerimos de una indexaci\'on de los estos.

	Para desarrollar una medida de asociaci\'on entre vectores, $x$ e $y$, modificamos la definici\'on de $I(r)$ como sigue. Sean $z_i=(x_i,y_i)$ con $i=1,\dots, N$ las observaciones en el conjunto de datos. Con esto definimos:

	\begin{defn}[Correlaci\'on Integral entre vectores]
		Sean $x= \{x_1,\dots,x_N\} $ e $y=\{y_1,\dots,y_N\}$ dos vectores de $N$ observaciones y sea $r\in\R_{>0}$. Definimos la integral de correlaci\'on $\hat{I}(r)$ entre $x$ e $y$ como:
		$$\hat{I}(r)=\frac{1}{N^{2}} \sum_{i, j=1}^{N} I\left(\left|\begin{pmatrix}
			x_{i} \\
			y_{i}
		  \end{pmatrix}-\begin{pmatrix}
			x_{j} \\
			y_{j}
		  \end{pmatrix}\right|<r\right)$$,
		donde $|\cdot|$ es la norma euclidiana. En adelante, al referirnos a la Integral de Correlaci\'on nos referiremos a esta definici\'on, a menos que se indique lo contrario.

	\end{defn}
	
	Notemos que las distancias obsevadas son adem\'as linealmente transformadas para que se encuentren entres 0 y 1 antes de calcular $\hat{I}$. Es importante destacar que $\hat{I}$a tiene las propiedades de una funci\'on de distribuci\'on acumulativa. Es no decreciente entre 0 y 1 y continua por la derecha. La funci\'on $\hat{I}(r)$ descrive el patr\'on global de distancias entre vecinos. 

	El inter\'es principal es la definici\'on de una metrica para cuantificar la asosiaci\'on no lineal estudiando patrones locales. Dado esto, definimos la densidad de vecinos $D$ de forma similar a la derivada de $\hat{I}$: 
	\begin{defn} [Densidad de Vecinos]
		Dados $x= \{x_1,\dots,x_N\} $, $y=\{y_1,\dots,y_N\}$ y su respectiva Integral de correalci\'on $\hat{I}(r)$
		$$
		\hat{D}(r)= \frac{\vartriangle\hat{I}(r)}{\vartriangle r},
		$$
		donde $\vartriangle\hat{I}(r)$ denota un cambio en $\hat{I}(r)$ y $\vartriangle r$ la magnitud de este.
	\end{defn}
	
	La densidad de vecinos observada es evaluada en radio distreto r, con $r=0,1/m, 2/m, \dots, 1$, con $m$ es un grosor de malla arbitrario que determina $\vartriangle r = \frac{1}{m}$. Una funci\'on de suavizado autom\'atico es usado para suavizar $D(r)$, en su propuesta original se us\'o el algoritmo propuesto por Videla et al. (2007) \cite{videla2007}, nosotros utilizaremos filtros Savitzky-Golay \cite{Savitzky-Golay}, tambi\'en haciendo uso de validaci\'on cruzada para elegir los mejores hiperparametros. En el paper, el tama\~no predeterminado $m$ se establece como $N$, el n\'umero de observaciones y en este trabajo usaremos el mismo $m$. El estad\'istico $\hat{D}$ es una aproximaci\'on discreta de $d\hat{I}(r)/d r$, la cual tiene las propiedades formales de una probabilidad funci\'on de densidad. Por lo tanto, con un ligero abuso de terminolog\'ia nos referimos a $\widehat{D}(r)$ como una distribuci\'on.

	En base a esto definimos la correlaci\'on local. Intuitivamente, las distancias entre los puntos de datos entre dos variables correlacionadas diferir\'ian de las distancias entre dos variables no correlacionadas. 
	
	\begin{defn}[Correlaci\'on Local]
	
		Sea $\widehat{D_0}(r)$ la estimaci\'on de una distribuci\'on nula, que se compone de dos vectores sin asociaci\'on. Definimos la correlaci\'on local ($\ell(r)$) como la desviaci\'on de D de la de la distribuci\'on nula a una distancia vecina dada r:

		$$
			\ell(r)=\widehat{D}(r)-\widehat{D}_{0}(r),
		$$	

		donde $\widehat{D}(r)$ es la distribuci\'on asociada a los vectores que queremos comparar.

	\end{defn}

	Este enfoque no asume ninguna distribuci\'on param\'etrica. La flexibilidad de este m\'etodo facilita el cambio de la distribuci\'on nula a cualquier distribuci\'on de inter\'es. 
	
	Por ultimo, definimos el coficiente como de correlaci\'on local m\'axima, o coeficiente de Chen como:

	\begin{defn}
		Sea $\ell(r)$ la funci\'on de correlaci\'on local entre dos vectores, definimos el coeficiente de correlaci\'on local m\'axima como:

		$$
		M=\max _{r}\{|\ell(r)|\}
		$$

		con $r\in[0,1]$. Recordemos que las distancias fueron nomralizadas.
		
	\end{defn}
	La interpretaci\'on de $\ell(r)$ como la diferencia de dos distribuciones implica que $M$ puede interpretarse como la distancia bajo la norma del supremo entre $\widehat{D}$ y $\widehat{D_0}$. En otras palabras, definimos el estad\'istico M como la desviaci\'on m\'axima entre dos densidades vecinas subyacentes.

\section[]{\textit{Distance Covariance \& Distance Correlation}} 

\subsection{Introducci\'on}


La Covarianza por Distancia  y la Correlaci\'on por distancia  fueron propuestas por Sz\'ekely, Rizzo, y Bakirov en su trabajo \textit{''Measuring and testing independence by correlation of distances''} (2007) \cite{Szekely2007}, posteriormente tambi\'en propusieton la Covarianza por Distancia Browniana en su trabajo \textit{''Brownian Distance Covariance''} (2009) \cite{Szekely2009}, en donde tambi\'en se demostr\'o que esta coincide con la Covarianza por Distancia tradicional.

De la misma forma que las medidas que hemos visto anteriormente, estas son medidas de asociaci\'on no param\'etricas que buscan encontrar relaciones no lineales entre dos conjuntos de datos, en particular establecer una forma de caracterizar independenc\'ia entre dos distribuci\'ones. Dado esto es importante recalcar que, para distribuci\'on de primer momento fin\'ito, la Correlaci\'on por distancia ($\mathcal{R}$) generaliza la idea de correalci\'on en dos formas:

\begin{enumerate}
	\item $\mathcal{R}(X,Y)$ est\'a definido para $X,Y$ de dimensi\'on aleatoria.
	\item $\mathcal{R}(X,Y) = 0$ caracteriza la independenc\'a de $X$ e $Y$.	 
\end{enumerate}

La primera de estas afirmaciones es importante, ya que nos permite utulizar esta medida para comparar im\'agenes sin mayor problema, de todas formas en la secci\'on \ref{chap5} estudiaremos como adaptar est\'as medidas para im\'agnes. En esta secci\'on definiremos estos 3 coeficientes, la relaci\'on entre ellos y verificaremos la afirmaciones realizadas m\'as arriba, para esto comenzaremos con las definiciones. 

\subsection{Definiciones}

Sean $X$ en $\mathbb{R}^p$ y $Y$ en $\mathbb{R}^q$ vectores aleatorios, donde $p$ y $q$ son enteros positivos. Las funciones en min\'uscula $f_X$ y $f_Y$ se utilizarán para denotar las funciones caracter\'isticas de $X$ y $Y$, respectivamente, y su funci\'on caracter\'istica conjunta se denota como $f_{X, Y}$. 

\begin{defn}[Covarianza por distancia]
	La Covarianza por distancia (dCov) entre los vectores aleatorios $X$ e $Y$ con primeros momentos finitos es el n\'umero no negativo $\mathcal{V}(X, Y)$ definido por:
	$$
	\begin{aligned}
	\mathcal{V}^2(X, Y) & =\left\|f_{X, Y}(t, s)-f_X(t) f_Y(s)\right\|^2 \\
	& =\frac{1}{c_p c_q} \int_{\mathbb{R}^{p+q}} \frac{\left|f_{X, Y}(t, s)-f_X(t) f_Y(s)\right|^2}{|t|_p^{1+p}|s|_q^{1+q}} d t d s .
	\end{aligned}
	$$
	\end{defn}

	Similarmente, la Varianza por distancia (dVar) se define como la raiz cuadrada:

	$$
	\mathcal{V}^2(X)=\mathcal{V}^2(X, X)=\left\|f_{X, X}(t, s)-f_X(t) f_X(s)\right\|^2 .
	$$

	Por definici\'on de la norma $\|\cdot\|$, es claro que  $\mathcal{V}(X, Y) \geq 0$ y $\mathcal{V}(X, Y)=0$ si y solo si $X$ e $Y$ son indepentes.

	\begin{defn} [Correlaci\'on por distancia]
		La correlaci\'on por distancia (dCor) entre los vectores aleatorios $X$ e $Y$ con primer momento finito es el n\'unero no negtivo $\mathcal{R}(X, Y)$ definido por:
		$$
		\mathcal{R}^2(X, Y)= \begin{cases}\frac{\mathcal{V}^2(X, Y)}{\sqrt{\mathcal{V}^2(X) \mathcal{V}^2(Y)}}, & \mathcal{V}^2(X) \mathcal{V}^2(Y)>0 \\ 0, & \mathcal{V}^2(X) \mathcal{V}^2(Y)=0\end{cases}.
		$$

	Algunas propiedades de $\mathcal{R}$ an\'alogo a $\rho$ ser\'an revisadas m\'as adelante. 
	\end{defn}

	Ahora, para el caso muestral, definimos los estad\'isticos de distancia de la siguiente forma. Para una muestra aleatoria $(\mathbf{X}, \mathbf{Y})=\left\{\left(X_k, Y_k\right): k=1, \ldots, n\right\}$ de $n$ ventores aleatorios $(X, Y)$ i.i.d., calculamos las matrices de distancia euclideana  $\left(a_{k l}\right)=\left(\left|X_k-X_l\right|_p\right)$ y $\left(b_{k l}\right)=\left(\left|Y_k-Y_l\right|_q\right)$. Definimos:

	$$
	A_{k l}=a_{k l}-\bar{a}_{k .}-\bar{a}_{. l}+\bar{a}_{. .}, \quad k, l=1, \ldots, n,
	$$
	donde
	$$
	\bar{a}_{k .}=\frac{1}{n} \sum_{l=1}^n a_{k l}, \quad \bar{a}_{. l},=\frac{1}{n} \sum_{k=1}^n a_{k l}, \quad \bar{a}_{. .}=\frac{1}{n^2} \sum_{k, l=1}^n a_{k l} .
	$$
	
	De forma similar, definimos $B_{k l}=b_{k l}-\bar{b}_{k .}-\bar{b}_{\cdot l}+\bar{b}_{. .}$, para $k, l=1, \ldots, n$.

	\begin{defn}[Covarianza y Correlaci\'on por distancia muestral]
		La no negativa Covarianza por distancia muestral $\mathcal{V}_n(\mathbf{X}, \mathbf{Y})$ y la Correlaci\'on por distancia muestral $\mathcal{R}_n(\mathbf{X}, \mathbf{Y})$ estan definidas por
		$$
		\mathcal{V}_n^2(\mathbf{X}, \mathbf{Y})=\frac{1}{n^2} \sum_{k, l=1}^n A_{k l} B_{k l},
		$$
		y
		$$
		\mathcal{R}_n^2(\mathbf{X}, \mathbf{Y})= \begin{cases}\frac{\mathcal{V}_n^2(\mathbf{X}, \mathbf{Y})}{\sqrt{\mathcal{V}_n^2(\mathbf{X}) \mathcal{V}_n^2(\mathbf{Y})}}, & \mathcal{V}_n^2(\mathbf{X}) \mathcal{V}_n^2(\mathbf{Y})>0 \\ 0, & \mathcal{V}_n^2(\mathbf{X}) \mathcal{V}_n^2(\mathbf{Y})=0\end{cases},
		$$
		respectivamente. Adem\'as la Varianza por distancia muestral $\mathcal{V}_n(\mathbf{X})$ est\'a definida por
		$$
		\mathcal{V}_n^2(\mathbf{X})=\mathcal{V}_n^2(\mathbf{X}, \mathbf{X})=\frac{1}{n^2} \sum_{k, l=1}^n A_{k l}^2 .
		$$
	\end{defn}

	\subsection{Covarianza Browniana}
		
		Para introducir la noci\'on de covarianza browniana, comenzaremos considerando la covarianza cuadrada del producto-momento. Recordemos que una variable prima $X'$ denota una copia independiente e id\'enticamente distribuida (i.i.d.) del símbolo no prima $X$. Para dos variables aleatorias continuas de valores reales, el cuadrado de su covarianza clásica es:

		$$\E[(X-\E(X))(Y-\E(Y))] = \E[(X-\E(X))(X'-\E(X'))(Y-\E(Y))(Y'-\E(Y'))]	$$

		Ahora generalizamos la covarianza al cuadrado y definimos el cuadrado de la covarianza condicional, dadas dos procesos estocásticos de valores reales, $U(\circ)$ y $V(*)$. Obtenemos un resultado interesante cuando U y V son procesos de Wiener independientes.


		Primero, para centrar la variable aleatoria $X$ en la covarianza condicional, necesitamos la siguiente definici\'on. Sea $X$ una variable aleatoria de valores reales y $\{U(t): t \in \R\}$ un proceso estoc\'astico de valores reales, independiente de $X$. La versi\'on centrada de $X$ respecto a $U$ se define por:

		$$
		X_U = U(X)-\int_{-\infty}^{\infty}U(t)dF_X(t)=U(X)-\E[U(X)|U],
		$$

		siempre que la esperanza condicional exista. Notemos que, si $id$ es la identidad, $X_{id}=X-\E[X]$. 

		Ahora, sea $\mathcal{W}$ un movimiento Browniano/Wiener de una dimensi\'on en ambos lados con esperanza cero y funci\'on de covarianza.
		\begin{equation}\label{brw_covariamce}
			|s|+|t|-|s-t|=2\min(s.t)
		\end{equation}
			

		Esto es dos veces la covarianza estandar para un proceso Wiener. Pero en adelante el factor de dos nos simplificar\'a algunos calculos, por lo que en el resto de la secci\'on asumiermos esta funcion de covarianza \ref{brw_covariamce} para $\mathcal{W}$. Ya con esto, podemos definir la Covarianza Browniana 

		\begin{defn}[Covarianza Browniana]
			La Covarianza Browniana (o covarianza Wiener) de dos variables aleatorias con valores reales $X$ e $Y$, con segundos monetos fin\'itos es el n\'umero no negativo definido por su cuadrado:
			\begin{equation}\label{brw_cov}
				\mathcal{W}^2(X,Y)={Cov}_{{W}^2}(X,Y)=\E[X_{W}X'_{W}Y_{W'}Y'_{W'}],
			\end{equation}

			donde $({W},{W'})$ no dependen de $(X,X',Y.Y')$.
		\end{defn}

		Tomemos en cuenta que si ${W}$ en ${Cov}_{{W}^2}$ es reemplazada por la funci\'on identidad (no aleatoria) $id$, entonces ${Cov}_{id}(X, Y) = |{Cov}(X, Y)| = |\sigma_{X,Y}|$, el valor absoluto de la covarianza del producto-momento de Pearson. Mientras que la covarianza del producto-momento estandarizada, la correlación de Pearson ($\rho$), mide el grado de relaci\'on lineal entre dos variables con valores reales, veremos que la covarianza estandarizada de Brownian mide el grado de todos los tipos posibles de relaciones entre dos variables aleatorias con valores reales.

		La definición de ${Cov}_{{W}^2}$ se puede extender a procesos aleatorios en dimensiones superiores de la siguiente manera. Si $X$ es una variable aleatoria con valores en $\R^p$ y $U(s)$ es un proceso aleatorio (campo aleatorio) definido para todos los $s \in \R^p$ e independiente de $X$, define la versión centrada en $U$ de $X$ como

		$$
			X_U=\E(X)-\E[U(X)|U]
		$$
		siempre que la expectativa condicional exista.

		\begin{defn}
			Si $X$ es un vector aleatorio con valores en $\R^p$ y $Y$ es un vector aleatorio con valores en $\R^q$, y $U(s)$ y $V(t)$ son procesos aleatorios, definamos para todo $s\in\R^p$, $t\in\R^q$, entonces la covarianza (U,V) de $X$ e $Y$ es el n\'umero no negativo definido por su cuadrado:
			$$
				{Cov}^2_{(U,V)}(X,Y) = \E[X_U X'_U Y_V Y'_V],
			$$
			siempre que el lado derecho sea positivo y finito.
		\end{defn}

		En particular, si ${W}$ y ${W'}$ son procesos brownianos con funci\'on de covarianza \ref{brw_covariamce} en $\R^p$ y $\R^q$ respectivamente, la Covarianza Browniana de $X$ e $Y$ esta definida por:

		\begin{equation}
			\mathcal{W}^2(X,Y) = Cov^2_W(X,Y) = Cov^2_{W,W'}(X,Y).
		\end{equation}

		De la misma forma, para variables aleatorias con varianza finita podemos definir la Varianza Browniana:

		\begin{equation}
			\mathcal{W}(X) = Cov_W(X) = Cov_{W}(X,X).
		\end{equation}
		
		Y con esto, podemos definir la correlaci\'on Browniana:
		
		\begin{defn}[Correlaci\'on Browniana]
			Definimos la correalci\'on Browniana de dos variables aleatorias con valores reales $X$ e $Y$ con segundos momentos finitos como:
			\begin{equation}	
				Cor_W(X,Y) = \frac{\mathcal{W}}{\sqrt[]{\mathcal{W}(X)\mathcal{W}(Y)}},
			\end{equation}
			siempre que el denominador no sea 0; otro caso $Cor_W(X,Y) = 0$.
		\end{defn}

		Ahora, solo nos queda probar que $Cov_W(X,Y)$ existe para vectores aleatorios de segundos momentos finitos, y con esto derivar la Covarianza Browniana. Para esto, tenemos el siguiente teorema:

		\begin{thm}
			Si $X$ es un vector aleatorio con valores en $\R^p$, e $Y$ es un vector aleatorio con valores en $\R^q$, y $\E(|X|^2+|Y|^2)>\infty$, entonces $\E[X_{W}X'_{W}Y_{W'}Y'_{W'}]$ es no negativo y finito, y:
			\begin{align}
				\mathcal{W}^2(X,Y) &= \E[X_{W}X'_{W}Y_{W'}Y'_{W'}]\\
								   &= \E|X-X'||Y-Y'| + \E|X-X'|\E|Y-Y'| - \label{cov_browniana_exists} \\ 
								   &\qquad - \E|X-X'||Y-Y''|\E|X-X''||Y-Y'|,
 			\end{align}
			donde $(X,Y)$, $(X',Y')$, y $(X'',Y'')$ son iid.
		\end{thm}
		\begin{proof}
			Notemos que
			$$
			\begin{aligned}
			E\left[X_W X_W^{\prime} Y_{W^{\prime}} Y_{W^{\prime}}^{\prime}\right] & =E\left[E\left(X_W Y_{W^{\prime}} X_W^{\prime} Y_{W^{\prime}}^{\prime} \mid W, W^{\prime}\right)\right] \\
			& =E\left[E\left(X_W Y_{W^{\prime}} \mid W, W^{\prime}\right) E\left(X_W^{\prime} Y_{W^{\prime}}^{\prime} \mid W, W^{\prime}\right)\right] \\
			& =E\left[E\left(X_W Y_{W^{\prime}} \mid W, W^{\prime}\right)\right]^2,
			\end{aligned}
			$$
			y esto es siempre no negativo. Para finitud, es suficiente demostrar que todos los factores en la definici\'on de  $\operatorname{Cov}_W(X, Y)$ tienen cuartos momentos finitos. La ecuaci\'on \ref{cov_browniana_exists} se basa en la forma especial de la funci\'on de covarianza de \ref{brw_covariamce} de $W$. El resto de la demostraci\'on se puede encontrar en el ap\'endice de /Szehely and Rizzo (2009)\cite{Szekely2009}.	
		\end{proof}

	\subsection{Equivalencia entre covarianzas}
	3.3. The surprising coincidence: $\mathcal{W}=\mathcal{V}$.

	THEOREM 8. For arbitrary $X \in \mathbb{R}^p, Y \in \mathbb{R}^q$ with finite second moments
	$$
	\mathcal{W}(X, Y)=\mathcal{V}(X, Y) .
	$$
	
	Proof. Both $\mathcal{V}$ and $\mathcal{W}$ are nonnegative, hence, it is enough to show that their squares coincide. Lemma 1 can be applied to evaluate $\mathcal{V}^2(X, Y)$. In the numerator of the integral we have terms like
	$$
	E\left[\cos \left\langle X-X^{\prime}, t\right\rangle \cos \left\langle Y-Y^{\prime}, s\right\rangle\right],
	$$
	where $X, X^{\prime}$ are i.i.d. and $Y, Y^{\prime}$ are i.i.d. Now apply the identity
	$$
	\cos u \cos v=1-(1-\cos u)-(1-\cos v)+(1-\cos u)(1-\cos v)
	$$
	and Lemma 1 to simplify the integrand. After cancelation in the numerator of the integrand, there remains to evaluate integrals of the type
	$$
	\begin{aligned}
	& E \int_{\mathbb{R}^{p+q}} \frac{\left.\left[1-\cos \left\langle X-X^{\prime}, t\right\rangle\right]\left[1-\cos \left\langle Y-Y^{\prime}, s\right\rangle\right)\right]}{|t|^{1+p}|s|^{1+q}} d t d s \\
	& \quad=E\left[\int_{\mathbb{R}^p} \frac{1-\cos \left\langle X-X^{\prime}, t\right\rangle}{|t|^{1+p}} d t \times \int_{\mathbb{R}^q} \frac{1-\cos \left\langle Y-Y^{\prime}, s\right\rangle}{|s|^{1+q}} d s\right] \\
	& =c_p c_q E\left|X-X^{\prime}\right| E\left|Y-Y^{\prime}\right|
	\end{aligned}
	$$
	
	Applying similar steps, after further simplification, we obtain
	$$
	\begin{aligned}
	\mathcal{V}^2(X, Y)= & E\left|X-X^{\prime}\right|\left|Y-Y^{\prime}\right|+E\left|X-X^{\prime}\right| E\left|Y-Y^{\prime}\right| \\
	& -E\left|X-X^{\prime}\right|\left|Y-Y^{\prime \prime}\right|-E\left|X-X^{\prime \prime}\right|\left|Y-Y^{\prime}\right|,
	\end{aligned}
	$$
	and this is exactly equal to the expression (3.7) obtained for $\mathcal{W}(X, Y)$ in Theorem 7 .
	As a corollary to Theorem 8, the properties of Brownian covariance for random vectors $X$ and $Y$ with finite second moments are therefore the same properties established for distance covariance $\mathcal{V}(X, Y)$ in Theorem 3.

	The surprising result that Brownian covariance equals distance covariance dCov, exactly as defined in (2.6) for $X \in \mathbb{R}^p$ and $Y \in \mathbb{R}^q$, parallels a familiar special case when $p=q=1$. For bivariate $(X, Y)$ we found that $\mathcal{R}(X, Y)$ is a natural counterpart of the absolute value of the Pearson correlation. That is, if in (3.5) $U$ and $V$ are the simplest nonrandom function $i d$, then we obtain the square of Pearson covariance $\sigma_{X, Y}^2$. Next, if we consider the most fundamental random processes, $U=W$ and $V=W^{\prime}$, we obtain the square of distance covariance, $\mathcal{V}^2(X, Y)$.

	Interested readers are referred to Székely and Bakirov [25] for the background of the interesting coincidence in Theorem 8 .
		\todo{Traducir esto}
\section[]{Correlaci\'on de Pearson}

	\subsection{Introducci\'on}

	\todo{donde se public\'o, pequena intro.}
		
	 
	\section{Definiciones}
	
	El coef. se define como:
	\begin{equation}\label{pearson_orig}
		\rho_{X,Y}=\frac{cov(X,Y)}{\sigma_X\sigma_Y}
	\end{equation}
	
	Para una muestra de tama\~no $N$, tenemos:
	
	\begin{equation}\label{pearson_r}
		r=\frac{\sum_{i}^N\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sqrt{\sum_{i}^n\left(x_{i}-\bar{x}\right)^{2}} \sqrt{\sum_{i}^n\left(y_{i}-\bar{y}\right)^{2}}}
	\end{equation}
	
	Con $x_i,y_i$ elementos de la muestra y $\bar{x},\bar{y}$ sus respectivos promedios.
	\todo{Hablar de The Ineffectiveness of the Correlation Coefficient for Image ComparisonsS}

\section{Ejemplos}

	\todo{usar datasaurus para mostrar los coeficientes}

	\newpage